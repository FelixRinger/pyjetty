# Config file for pp vs AA jet classification

#------------------------------------------------------------------------------
# These parameters are used in the process script
#------------------------------------------------------------------------------

K_max: 50
jetR: [0.2]
jet_pt_bins: [[40.,60.], [60.,80.], [80,120.]]
eta_max: 0.9
jet_matching_distance: 0.6        # Match jets with deltaR < jet_matching_distance*jetR
mc_fraction_threshold: 0.5
reject_tracks_fraction: 0.02
min_pt: 0.
leading_track_pt: 5.
photon_jet: False

#thermal_model: 
#  beta: 0.4
#  N_avg: 2500
#  sigma_N: 500

constituent_subtractor:
  max_distance: [0.25]
  alpha: 0
  bge_rho_grid_size: 1.0
  max_pt_correct: 100
  ghost_area: 0.01

emb_file_list: '/rstorage/alice/data/LHC18qr/570/files.txt'

#------------------------------------------------------------------------------
# All parameters below are only used in the analysis script
#------------------------------------------------------------------------------

# Define Nsubjettiness observable basis sets to train DNN 
# The K-body phase space is (3K-4)-dimensional
# Note: N-subjettiness plot only works up to K=6 (1810.05165 used K=24)
K: [5, 10, 20, 30, 40]

# Load labeled data
n_train: 20000
n_val: 4000
n_test: 4000

# Classification labels
pp_label: 'pp'
AA_label: 'Pb-Pb'

# Select model: linear, random_forest, neural_network, pfn, efn, lasso
models: [neural_network, pfn, efn, lasso]

linear:

    # Model hyperparameters
    loss: 'hinge'                # cost function
    penalty: ['l2', 'l1']        # regularization term
    alpha: [1e-5, 1e-4, 1e-3]    # regularization strength
    max_iter: 1000               # max number of epochs
    tol: [1e-5, 1e-4, 1e-3]      # criteria to stop training
    learning_rate: 'optimal'     # learning schedule (learning rate decreases over time in proportion to alpha)
    early_stopping: False        # whether to stop training based on validation score
    
    # Hyperparameter tuning
    n_iter: 10                   # number of random hyperparameter sets to try
    cv: 5                        # number of cross-validation folds
    
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

random_forest:

    # Model hyperparameters
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

neural_network:

    # Model hyperparameters
    learning_rate: 0.001         # (cf 1810.05165)
    loss: 'binary_crossentropy'  # loss function - use categorical_crossentropy instead ?
    metrics: ['accuracy']        # measure accuracy during training
    batch_size: 1000
    epochs: 30                   # number of training epochs
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)

pfn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    epochs: 30                   # number of training epochs
    batch_size: 500
    use_pids: True               # Use PID information
    
efn:

    # Network architecture parameters
    Phi_sizes: [100, 100, 256]
    F_sizes: [100, 100, 100]

    # Network training parameters
    learning_rate: 0.001
    epochs: 30                   # number of training epochs
    batch_size: 500
    
lasso:

    # Set K value to train Lasso on
    K_lasso: 20

    # Model hyperparameters
    alpha: [0.02, 0.05, 0.08]        # Constant multiplying the L1 term. 0 corresponds to the standard regression
    max_iter: 10000               # max number of epochs
    tol: 1e-6                     # criteria to stop training

    # Hyperparameter tuning
    n_iter: 3                    # number of random hyperparameter sets to try
    cv: 5                        # number of cross-validation folds
    
    random_state: null           # seed for shuffling data (set to an int to have reproducible results)
